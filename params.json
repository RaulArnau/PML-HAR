{"name":"Pml-har","tagline":"Coursera Practical Machine Learning Course. Final Project: Human Activity Recognition","body":"# Recognition of weight lifting exercises\r\n\r\n# Executive summary\r\nThis report describes the process followed to build a prediction model that infers the way a weight lifting exercise is performed. The data used to train the model are gathered from six participants that are being monitored by a set of accelerometers while they perform a barbel lifting exercise in five different ways (one right and four common mistakes).\r\n\r\nIn order to make this research fully reproducible, it contains the complete R-code necessary to get the data sets (which are available online), explore them, clean and prepare the data sets, build the model and use it to predict the outcome for 20 instances. The final model is a random forest predictor which obtains a very good out of sample accuracy (around 98%). The predictor is robust, trained using k-fold cross validation, which balances bias and variance trade-offs using the out of sample accuracy as a metric to choose the best model.\r\n\r\n# Getting the data\r\nThe following chunk of code prepares the work space, fetches the data sets and loads them in the environment:\r\n\r\n```r\r\nsuppressWarnings(library(caret))\r\nsuppressWarnings(library(ggplot2))\r\nrm(list=ls())\r\ndataUrl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\nvalidationUrl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\ndataFile <- \"pml-training.csv\"\r\nvalidationFile <- \"pml-testing.csv\"\r\n# download.file(dataUrl, dataFile, method='auto')\r\n# download.file(validationUrl, validationFile, method='auto')\r\ndataSet <- read.csv(dataFile, na.strings=c(\"NA\", \"\"))\r\nvalidationSet <- read.csv(validationFile, na.strings=c(\"NA\", \"\"))\r\nnumInstances <- dim(dataSet)[1]; numFeatures <- dim(dataSet)[2]\r\ndim(validationSet)\r\n```\r\n\r\n```\r\n## [1]  20 160\r\n```\r\nWith 160 features in each data set apart from the type of exercise and 19622 instances in the training set. \r\n\r\n# Exploratory analysis\r\nTaking a first look a the data we can see some of the (a priori) most representative features:\r\n\r\n```r\r\ndataSet[1:5, c('user_name', 'classe', 'num_window',\r\n               'roll_belt', 'pitch_belt', 'yaw_belt')]\r\n```\r\n\r\n```\r\n##   user_name classe num_window roll_belt pitch_belt yaw_belt\r\n## 1  carlitos      A         11      1.41       8.07    -94.4\r\n## 2  carlitos      A         11      1.41       8.07    -94.4\r\n## 3  carlitos      A         11      1.42       8.07    -94.4\r\n## 4  carlitos      A         12      1.48       8.05    -94.4\r\n## 5  carlitos      A         12      1.48       8.07    -94.4\r\n```\r\nThe following figure shows the class distribution for the data set:\r\n\r\n```r\r\nqplot(classe, fill=classe, data=dataSet)\r\n```\r\n\r\n![plot of chunk classDistribution](./PML-project_files/figure-html/classDistribution.png) \r\n\r\nIt can be seen that not all classes are equally sampled. It would be desirable to preserve the class distribution when splitting the data set into training and testing sets.\r\nBesides, the first five features are not interesting for prediction, apart from (maybe) the user name. Those features are:\r\n\r\n\r\n```r\r\nnames(dataSet)[c(1:5)[-2]]\r\n```\r\n\r\n```\r\n## [1] \"X\"                    \"raw_timestamp_part_1\" \"raw_timestamp_part_2\"\r\n## [4] \"cvtd_timestamp\"\r\n```\r\n\r\n# Cleaning data\r\n\r\nThe following code gets rid of non interesting features and cast the rest of them as numeric values. Factor variables, such as the user name, are also converted to numeric:\r\n\r\n\r\n```r\r\n# convert interesting features to numeric values\r\ndataSetNum <- dataSet[, c(2, 7:159)] # 2: user name\r\nvalidationSetNum <- validationSet[, c(2, 7:159)]\r\n# Store class in a separate variable for simplicity\r\ny <- dataSet$classe\r\n\r\ndataSetNum <- data.frame(sapply(dataSetNum, as.numeric))\r\nvalidationSetNum <- data.frame(sapply(validationSetNum, as.numeric))\r\n```\r\n\r\nThe data set contains a huge percent of NA values:\r\n\r\n\r\n```r\r\nmean(is.na(dataSetNum))\r\n```\r\n\r\n```\r\n## [1] 0.6359\r\n```\r\n\r\n```r\r\nhist(100*colMeans(is.na(dataSetNum)), col='lightblue', \r\n     xlab='NA occurrences', ylab='num features', \r\n     main = \"Number of NA's (in %)\")\r\n```\r\n\r\n![plot of chunk naValues](./PML-project_files/figure-html/naValues.png) \r\n\r\nAnalyzing the NA distribution it can be seen that there are only two cases: those features which contain no NA's, and those that do. In the seconds case, when there are NA's in the data, they are present in almost the 100% of the instances. We could remove those features since they are not going to be useful for prediction:\r\n\r\n\r\n```r\r\ncompleteFeatures <- complete.cases(t(dataSetNum))\r\ndataSetNum <- dataSetNum[, completeFeatures]\r\nvalidationSetNum <- validationSetNum[, completeFeatures]\r\n```\r\n\r\nThis could also be done using the `nearZeroVar()` function, but in this case it was pretty clear which features would have zero variance. \r\n\r\n\r\n# Data partitioning\r\n\r\nThe data set labeled with the type of exercise (classe) is split into a training and a testing dataset. The training set will be used to train the predictor and the testing to evaluate its out of sample accuracy. The following code splits the data giving a 75% to the  training set:\r\n\r\n\r\n\r\n```r\r\n# Split training set into training and testing to build our predictor\r\nset.seed(3232)\r\ninTrain <- createDataPartition(y=y, p=0.75, list=FALSE)\r\ntraining <- dataSetNum[inTrain, ]\r\ntesting <- dataSetNum[-inTrain, ]\r\ndim(training)\r\n```\r\n\r\n```\r\n## [1] 14718    54\r\n```\r\n\r\n```r\r\ndim(testing)\r\n```\r\n\r\n```\r\n## [1] 4904   54\r\n```\r\n\r\nBesides, we could also remove those features which present a high correlation in the training set, since they are not going to provide useful information:\r\n\r\n\r\n```r\r\n# remove highly correlated features\r\ncorIdx <- findCorrelation(cor(training))\r\ntraining <- training[, -corIdx]\r\ntesting <- testing[, -corIdx]\r\nvalidation <- validationSetNum[, -corIdx]\r\n```\r\n\r\nTo refine a bit more the number of features used for the model training, we run a principal components analysis on the training data set. From the new set of features we keep only those that explain a 95% of the variability:\r\n\r\n\r\n```r\r\n# use PCA to get rid of some features\r\n# keep only those that explain the 95% of the variability\r\nmodPca <- preProcess(training, method = 'pca', threshold = 0.95)\r\ntrainingPca <- predict(modPca, training)\r\ntestingPca <- predict(modPca, testing)\r\nvalidationPca <- predict(modPca, validation)\r\n\r\n# add the class to the features datasets\r\ntrainingPca$classe <- y[inTrain]\r\ntestingPca$classe <- y[-inTrain]\r\n```\r\n \r\nFinally the class is added back from the original data set.\r\n\r\n\r\n# Model training\r\nThe model selected is a random forest predictor. The training process uses k-fold cross validation in order to avoid over fitting the training data. Not using all the available training data for building the model is going to increase the predictor bias, but on the other hand it has provided better out of sample errors during the tests. The model is trained according to the following code:\r\n\r\n\r\n\r\n```r\r\nset.seed(32335)\r\nfitControl <- trainControl(method = \"repeatedcv\", \r\n                           # 5-fold CV repeated 5 times\r\n                           number = 5,\r\n                           repeats = 5)\r\nmodFit <- train(classe ~., \r\n                method='rf', \r\n                trControl = fitControl, \r\n                data = trainingPca)\r\n```\r\n\r\n```\r\n## Loading required package: randomForest\r\n```\r\n\r\n```\r\n## Warning: package 'randomForest' was built under R version 3.1.1\r\n```\r\n\r\n```\r\n## randomForest 4.6-10\r\n## Type rfNews() to see new features/changes/bug fixes.\r\n```\r\n\r\nThe convergence of the training process is shown in the following figure:\r\n\r\n\r\n```r\r\n# plotting the resampling profile\r\ntrellis.par.set(caretTheme())\r\nplot(modFit, col='purple', lw=1)\r\n```\r\n\r\n![plot of chunk plotResample](./PML-project_files/figure-html/plotResample.png) \r\n\r\n\r\n\r\n## Validation\r\nIn order to validate the model, it is used to predict the classes for the training and testing data sets:\r\n\r\n\r\n```r\r\n# in sample error\r\ntrainingPrediction <- predict(modFit, trainingPca)\r\nconfusionMatrix(trainingPca$classe, trainingPrediction)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 4185    0    0    0    0\r\n##          B    0 2848    0    0    0\r\n##          C    0    0 2567    0    0\r\n##          D    0    0    0 2412    0\r\n##          E    0    0    0    0 2706\r\n## \r\n## Overall Statistics\r\n##                                 \r\n##                Accuracy : 1     \r\n##                  95% CI : (1, 1)\r\n##     No Information Rate : 0.284 \r\n##     P-Value [Acc > NIR] : <2e-16\r\n##                                 \r\n##                   Kappa : 1     \r\n##  Mcnemar's Test P-Value : NA    \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity             1.000    1.000    1.000    1.000    1.000\r\n## Specificity             1.000    1.000    1.000    1.000    1.000\r\n## Pos Pred Value          1.000    1.000    1.000    1.000    1.000\r\n## Neg Pred Value          1.000    1.000    1.000    1.000    1.000\r\n## Prevalence              0.284    0.194    0.174    0.164    0.184\r\n## Detection Rate          0.284    0.194    0.174    0.164    0.184\r\n## Detection Prevalence    0.284    0.194    0.174    0.164    0.184\r\n## Balanced Accuracy       1.000    1.000    1.000    1.000    1.000\r\n```\r\n\r\n```r\r\n# out of sample error\r\ntestingPrediction <- predict(modFit, testingPca)\r\nconfusionMatrix(testingPca$classe, testingPrediction)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1385    2    3    4    1\r\n##          B   19  916   14    0    0\r\n##          C    3   15  833    3    1\r\n##          D    1    2   37  761    3\r\n##          E    0    4    0    6  891\r\n## \r\n## Overall Statistics\r\n##                                        \r\n##                Accuracy : 0.976        \r\n##                  95% CI : (0.971, 0.98)\r\n##     No Information Rate : 0.287        \r\n##     P-Value [Acc > NIR] : < 2e-16      \r\n##                                        \r\n##                   Kappa : 0.97         \r\n##  Mcnemar's Test P-Value : 6.02e-08     \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity             0.984    0.976    0.939    0.983    0.994\r\n## Specificity             0.997    0.992    0.995    0.990    0.998\r\n## Pos Pred Value          0.993    0.965    0.974    0.947    0.989\r\n## Neg Pred Value          0.993    0.994    0.987    0.997    0.999\r\n## Prevalence              0.287    0.191    0.181    0.158    0.183\r\n## Detection Rate          0.282    0.187    0.170    0.155    0.182\r\n## Detection Prevalence    0.284    0.194    0.174    0.164    0.184\r\n## Balanced Accuracy       0.990    0.984    0.967    0.986    0.996\r\n```\r\n\r\nIt can be seen that the out of sample error is quite good, reaching an accuracy around the 98%.\r\n\r\n\r\n\r\n# Predictions\r\nFinally the trained model is used to predict the classes for the unlabeled data set. The obtained predictions are stored in separate files as required:\r\n\r\n\r\n```r\r\n# Predict validation set\r\nprediction <- predict(modFit, validationPca)\r\n# save results in the submission format\r\npml_write_files = function(x) {\r\n    n = length(x)\r\n    for (i in 1:n) {\r\n        filename = paste0(\"problem_id_\", i, \".txt\")\r\n        write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, \r\n                    col.names = FALSE)\r\n        }\r\n    }\r\npml_write_files(prediction)\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}